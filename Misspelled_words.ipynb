{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YQB5JHcy7-Hc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lod33r6zuVKX",
        "outputId": "2de65c61-8152-424b-c283-4227b78d6c48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        }
      ],
      "source": [
        "words = None\n",
        "if False:\n",
        "    from nltk.corpus import gutenberg as corpus\n",
        "    words = corpus.words()\n",
        "    vocabulary = vocabulary[803:-2]\n",
        "    # first 803 and last 2 words are punctuation signs, numbers and underscored\n",
        "    # words like _home_\n",
        "else:\n",
        "    # Using the whole corpus means too much work to later compute the words at a\n",
        "    # distance for each word in the vocabulary, so we load just one book\n",
        "    import nltk\n",
        "    nltk.download('gutenberg')\n",
        "    words = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "    # using all the words in the book will take 25-30 min. to process later\n",
        "    # so we limit its number for the moment.\n",
        "    words = words[:10000] # cell [15] will take 1.5 min.\n",
        "    #words = words[:100000] # cell [15] will take 15 min.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtZbq2hU7-Hj",
        "outputId": "ee9ed350-1852-4a11-a567-a765e8ac3a25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['A' 'Abbey' 'After' ... 'yourself' 'youth' 'youthful']\n",
            "Vocabulary size 1711 words\n"
          ]
        }
      ],
      "source": [
        "vocabulary = list(set(words))\n",
        "vocabulary.sort()\n",
        "# get rid of some non-words like ',' '--' '['\n",
        "idx_first_word = vocabulary.index('A')\n",
        "vocabulary = vocabulary[idx_first_word:]\n",
        "# plus some more annoying non-words\n",
        "vocabulary.remove('[')\n",
        "vocabulary.remove(']')\n",
        "vocabulary.remove('`')\n",
        "vocabulary.remove('II')\n",
        "vocabulary = np.array(vocabulary)\n",
        "print(vocabulary)\n",
        "print('Vocabulary size {} words'.format(len(vocabulary)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3CoMFvT7-Hk"
      },
      "source": [
        "For each word in the vocabulary find the nearest words = at Levenshtein distance up to ``MAX_DIST``. This is a long computation, $O(n^2)$ for $n$ size of the vocabulary. We try to speed up it a little : if $\\text{dist}(w_1, w_2) \\leq d$ then $|\\text{len}(w_1) - \\text{len}(w_2)| \\leq d$. This reduces the candidate words in the vocabulary for which to compute the distance to each word.\n",
        "We save the resulting dictionary to avoid recomputing it each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQSBCKoC7-Hm",
        "outputId": "fa671881-ac0d-4637-9627-91d5f92dc0d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1711/1711 [00:59<00:00, 28.72it/s]\n"
          ]
        }
      ],
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "def levenshtein(s1, s2):\n",
        "    return edit_distance(s1, s2, substitution_cost=1, transpositions=True)\n",
        "\n",
        "word_lengths = np.array([len(w) for w in vocabulary])\n",
        "dict_lengths = {}\n",
        "for l in range(min(word_lengths), max(word_lengths)+1):\n",
        "    dict_lengths[l] = vocabulary[word_lengths==l] # needs vocabulary to be a numpy array\n",
        "\n",
        "min_length = min(dict_lengths.keys())\n",
        "max_length = max(dict_lengths.keys())\n",
        "\n",
        "MAX_DIST = 1\n",
        "fname = 'close_words_{}.pkl'.format(MAX_DIST)\n",
        "if not os.path.exists(fname):\n",
        "    close_words = {}\n",
        "    for word in tqdm(vocabulary):\n",
        "        length = len(word)\n",
        "        candidate_words = []\n",
        "        d1 = max(min_length, length - MAX_DIST)\n",
        "        d2 = min(max_length, length + MAX_DIST)\n",
        "        for d in range(d1, d2+1):\n",
        "            candidate_words.extend(dict_lengths[d])\n",
        "        close_words[word] = [w for w in candidate_words if levenshtein(word,w) <= MAX_DIST]\n",
        "\n",
        "    with open(fname,'wb') as f:\n",
        "        pickle.dump(close_words, f)\n",
        "else:\n",
        "    close_words = pickle.load(open(fname,'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIdE_y_Z7-Ho"
      },
      "source": [
        "Given one sentence $X$, which is a list of words, build the candidates to correct sentence $C(X)$ assuming at most one word is mispelled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-GOhyl37-Hp",
        "outputId": "2479278e-bebc-4b48-85da-67d403ec867c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I\twish\tyou\twhere\there\n",
            "A\twish\tyou\twhere\there\n",
            "a\twish\tyou\twhere\there\n",
            "s\twish\tyou\twhere\there\n",
            "IV\twish\tyou\twhere\there\n",
            "If\twish\tyou\twhere\there\n",
            "In\twish\tyou\twhere\there\n",
            "It\twish\tyou\twhere\there\n",
            "I\tfish\tyou\twhere\there\n",
            "I\twith\tyou\twhere\there\n",
            "I\twish\tYou\twhere\there\n",
            "I\twish\tyour\twhere\there\n",
            "I\twish\tyou\there\there\n",
            "I\twish\tyou\twere\there\n",
            "I\twish\tyou\tThere\there\n",
            "I\twish\tyou\tWhere\there\n",
            "I\twish\tyou\tthere\there\n",
            "I\twish\tyou\twhere\ther\n",
            "I\twish\tyou\twhere\thers\n",
            "I\twish\tyou\twhere\twere\n",
            "I\twish\tyou\twhere\tThere\n",
            "I\twish\tyou\twhere\tWhere\n",
            "I\twish\tyou\twhere\tthere\n",
            "I\twish\tyou\twhere\twhere\n"
          ]
        }
      ],
      "source": [
        "#sentence = 'Only two of the apples'\n",
        "sentence = 'I wish you where here'\n",
        "X = sentence.split(' ')\n",
        "for x in X:\n",
        "  assert x in vocabulary, 'All the words in the sentence must belong to the '\\\n",
        "      + 'vocabulary, {} doesn\\'t'.format(x)\n",
        "CX = [X] # no errors\n",
        "for i in range(len(X)): # one mispelled word at a time\n",
        "    if X[i] in vocabulary:\n",
        "        for cw in close_words[X[i]]:\n",
        "            if cw != X[i]:\n",
        "                C = X.copy()\n",
        "                C[i] = cw\n",
        "                CX.append(C)\n",
        "    else:\n",
        "        pass # let it be as is\n",
        "for W in CX:\n",
        "    print('\\t'.join(W))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utoKOHP4VkTh",
        "outputId": "621f0015-913f-4b9d-b4da-4e3491f8034b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I\twish\tyou\twhere\there\n",
            "A\twish\tyou\twhere\there\n",
            "A\tfish\tyou\twhere\there\n",
            "A\twith\tyou\twhere\there\n",
            "A\twish\tYou\twhere\there\n",
            "A\twish\tyour\twhere\there\n",
            "A\twish\tyou\there\there\n",
            "A\twish\tyou\twere\there\n",
            "A\twish\tyou\tThere\there\n",
            "A\twish\tyou\tWhere\there\n",
            "A\twish\tyou\tthere\there\n",
            "A\twish\tyou\twhere\ther\n",
            "A\twish\tyou\twhere\thers\n",
            "A\twish\tyou\twhere\twere\n",
            "A\twish\tyou\twhere\tThere\n",
            "A\twish\tyou\twhere\tWhere\n",
            "A\twish\tyou\twhere\tthere\n",
            "A\twish\tyou\twhere\twhere\n",
            "a\twish\tyou\twhere\there\n",
            "a\tfish\tyou\twhere\there\n",
            "a\twith\tyou\twhere\there\n",
            "a\twish\tYou\twhere\there\n",
            "a\twish\tyour\twhere\there\n",
            "a\twish\tyou\there\there\n",
            "a\twish\tyou\twere\there\n",
            "a\twish\tyou\tThere\there\n",
            "a\twish\tyou\tWhere\there\n",
            "a\twish\tyou\tthere\there\n",
            "a\twish\tyou\twhere\ther\n",
            "a\twish\tyou\twhere\thers\n",
            "a\twish\tyou\twhere\twere\n",
            "a\twish\tyou\twhere\tThere\n",
            "a\twish\tyou\twhere\tWhere\n",
            "a\twish\tyou\twhere\tthere\n",
            "a\twish\tyou\twhere\twhere\n",
            "s\twish\tyou\twhere\there\n",
            "s\tfish\tyou\twhere\there\n",
            "s\twith\tyou\twhere\there\n",
            "s\twish\tYou\twhere\there\n",
            "s\twish\tyour\twhere\there\n",
            "s\twish\tyou\there\there\n",
            "s\twish\tyou\twere\there\n",
            "s\twish\tyou\tThere\there\n",
            "s\twish\tyou\tWhere\there\n",
            "s\twish\tyou\tthere\there\n",
            "s\twish\tyou\twhere\ther\n",
            "s\twish\tyou\twhere\thers\n",
            "s\twish\tyou\twhere\twere\n",
            "s\twish\tyou\twhere\tThere\n",
            "s\twish\tyou\twhere\tWhere\n",
            "s\twish\tyou\twhere\tthere\n",
            "s\twish\tyou\twhere\twhere\n",
            "IV\twish\tyou\twhere\there\n",
            "IV\tfish\tyou\twhere\there\n",
            "IV\twith\tyou\twhere\there\n",
            "IV\twish\tYou\twhere\there\n",
            "IV\twish\tyour\twhere\there\n",
            "IV\twish\tyou\there\there\n",
            "IV\twish\tyou\twere\there\n",
            "IV\twish\tyou\tThere\there\n",
            "IV\twish\tyou\tWhere\there\n",
            "IV\twish\tyou\tthere\there\n",
            "IV\twish\tyou\twhere\ther\n",
            "IV\twish\tyou\twhere\thers\n",
            "IV\twish\tyou\twhere\twere\n",
            "IV\twish\tyou\twhere\tThere\n",
            "IV\twish\tyou\twhere\tWhere\n",
            "IV\twish\tyou\twhere\tthere\n",
            "IV\twish\tyou\twhere\twhere\n",
            "If\twish\tyou\twhere\there\n",
            "If\tfish\tyou\twhere\there\n",
            "If\twith\tyou\twhere\there\n",
            "If\twish\tYou\twhere\there\n",
            "If\twish\tyour\twhere\there\n",
            "If\twish\tyou\there\there\n",
            "If\twish\tyou\twere\there\n",
            "If\twish\tyou\tThere\there\n",
            "If\twish\tyou\tWhere\there\n",
            "If\twish\tyou\tthere\there\n",
            "If\twish\tyou\twhere\ther\n",
            "If\twish\tyou\twhere\thers\n",
            "If\twish\tyou\twhere\twere\n",
            "If\twish\tyou\twhere\tThere\n",
            "If\twish\tyou\twhere\tWhere\n",
            "If\twish\tyou\twhere\tthere\n",
            "If\twish\tyou\twhere\twhere\n",
            "In\twish\tyou\twhere\there\n",
            "In\tfish\tyou\twhere\there\n",
            "In\twith\tyou\twhere\there\n",
            "In\twish\tYou\twhere\there\n",
            "In\twish\tyour\twhere\there\n",
            "In\twish\tyou\there\there\n",
            "In\twish\tyou\twere\there\n",
            "In\twish\tyou\tThere\there\n",
            "In\twish\tyou\tWhere\there\n",
            "In\twish\tyou\tthere\there\n",
            "In\twish\tyou\twhere\ther\n",
            "In\twish\tyou\twhere\thers\n",
            "In\twish\tyou\twhere\twere\n",
            "In\twish\tyou\twhere\tThere\n",
            "In\twish\tyou\twhere\tWhere\n",
            "In\twish\tyou\twhere\tthere\n",
            "In\twish\tyou\twhere\twhere\n",
            "It\twish\tyou\twhere\there\n",
            "It\tfish\tyou\twhere\there\n",
            "It\twith\tyou\twhere\there\n",
            "It\twish\tYou\twhere\there\n",
            "It\twish\tyour\twhere\there\n",
            "It\twish\tyou\there\there\n",
            "It\twish\tyou\twere\there\n",
            "It\twish\tyou\tThere\there\n",
            "It\twish\tyou\tWhere\there\n",
            "It\twish\tyou\tthere\there\n",
            "It\twish\tyou\twhere\ther\n",
            "It\twish\tyou\twhere\thers\n",
            "It\twish\tyou\twhere\twere\n",
            "It\twish\tyou\twhere\tThere\n",
            "It\twish\tyou\twhere\tWhere\n",
            "It\twish\tyou\twhere\tthere\n",
            "It\twish\tyou\twhere\twhere\n",
            "I\tfish\tyou\twhere\there\n",
            "I\tfish\tYou\twhere\there\n",
            "I\tfish\tyour\twhere\there\n",
            "I\tfish\tyou\there\there\n",
            "I\tfish\tyou\twere\there\n",
            "I\tfish\tyou\tThere\there\n",
            "I\tfish\tyou\tWhere\there\n",
            "I\tfish\tyou\tthere\there\n",
            "I\tfish\tyou\twhere\ther\n",
            "I\tfish\tyou\twhere\thers\n",
            "I\tfish\tyou\twhere\twere\n",
            "I\tfish\tyou\twhere\tThere\n",
            "I\tfish\tyou\twhere\tWhere\n",
            "I\tfish\tyou\twhere\tthere\n",
            "I\tfish\tyou\twhere\twhere\n",
            "I\twith\tyou\twhere\there\n",
            "I\twith\tYou\twhere\there\n",
            "I\twith\tyour\twhere\there\n",
            "I\twith\tyou\there\there\n",
            "I\twith\tyou\twere\there\n",
            "I\twith\tyou\tThere\there\n",
            "I\twith\tyou\tWhere\there\n",
            "I\twith\tyou\tthere\there\n",
            "I\twith\tyou\twhere\ther\n",
            "I\twith\tyou\twhere\thers\n",
            "I\twith\tyou\twhere\twere\n",
            "I\twith\tyou\twhere\tThere\n",
            "I\twith\tyou\twhere\tWhere\n",
            "I\twith\tyou\twhere\tthere\n",
            "I\twith\tyou\twhere\twhere\n",
            "I\twish\tYou\twhere\there\n",
            "I\twish\tYou\there\there\n",
            "I\twish\tYou\twere\there\n",
            "I\twish\tYou\tThere\there\n",
            "I\twish\tYou\tWhere\there\n",
            "I\twish\tYou\tthere\there\n",
            "I\twish\tYou\twhere\ther\n",
            "I\twish\tYou\twhere\thers\n",
            "I\twish\tYou\twhere\twere\n",
            "I\twish\tYou\twhere\tThere\n",
            "I\twish\tYou\twhere\tWhere\n",
            "I\twish\tYou\twhere\tthere\n",
            "I\twish\tYou\twhere\twhere\n",
            "I\twish\tyour\twhere\there\n",
            "I\twish\tyour\there\there\n",
            "I\twish\tyour\twere\there\n",
            "I\twish\tyour\tThere\there\n",
            "I\twish\tyour\tWhere\there\n",
            "I\twish\tyour\tthere\there\n",
            "I\twish\tyour\twhere\ther\n",
            "I\twish\tyour\twhere\thers\n",
            "I\twish\tyour\twhere\twere\n",
            "I\twish\tyour\twhere\tThere\n",
            "I\twish\tyour\twhere\tWhere\n",
            "I\twish\tyour\twhere\tthere\n",
            "I\twish\tyour\twhere\twhere\n",
            "I\twish\tyou\there\there\n",
            "I\twish\tyou\twere\there\n",
            "I\twish\tyou\twere\ther\n",
            "I\twish\tyou\twere\thers\n",
            "I\twish\tyou\twere\twere\n",
            "I\twish\tyou\twere\tThere\n",
            "I\twish\tyou\twere\tWhere\n",
            "I\twish\tyou\twere\tthere\n",
            "I\twish\tyou\twere\twhere\n",
            "I\twish\tyou\tThere\there\n",
            "I\twish\tyou\tThere\ther\n",
            "I\twish\tyou\tThere\thers\n",
            "I\twish\tyou\tThere\twere\n",
            "I\twish\tyou\tThere\tThere\n",
            "I\twish\tyou\tThere\tWhere\n",
            "I\twish\tyou\tThere\tthere\n",
            "I\twish\tyou\tThere\twhere\n",
            "I\twish\tyou\tWhere\there\n",
            "I\twish\tyou\tWhere\ther\n",
            "I\twish\tyou\tWhere\thers\n",
            "I\twish\tyou\tWhere\twere\n",
            "I\twish\tyou\tWhere\tThere\n",
            "I\twish\tyou\tWhere\tWhere\n",
            "I\twish\tyou\tWhere\tthere\n",
            "I\twish\tyou\tWhere\twhere\n",
            "I\twish\tyou\tthere\there\n",
            "I\twish\tyou\tthere\ther\n",
            "I\twish\tyou\tthere\thers\n",
            "I\twish\tyou\tthere\twere\n",
            "I\twish\tyou\tthere\tThere\n",
            "I\twish\tyou\tthere\tWhere\n",
            "I\twish\tyou\tthere\tthere\n",
            "I\twish\tyou\tthere\twhere\n",
            "I\twish\tyou\twhere\ther\n",
            "I\twish\tyou\twhere\thers\n",
            "I\twish\tyou\twhere\twere\n",
            "I\twish\tyou\twhere\tThere\n",
            "I\twish\tyou\twhere\tWhere\n",
            "I\twish\tyou\twhere\tthere\n",
            "I\twish\tyou\twhere\twhere\n"
          ]
        }
      ],
      "source": [
        "#ex2\n",
        "sentence = 'I wish you where here'\n",
        "X = sentence.split(' ')\n",
        "\n",
        "for x in X:\n",
        "    assert x in vocabulary, 'All the words in the sentence must belong to the vocabulary, {} doesn\\'t'.format(x)\n",
        "\n",
        "CX = [X]  # Original sentence\n",
        "\n",
        "for i in range(len(X)):\n",
        "    if X[i] in vocabulary:\n",
        "        for cw1 in close_words[X[i]]:\n",
        "            if cw1 != X[i]:\n",
        "                C1 = X.copy()\n",
        "                C1[i] = cw1\n",
        "                CX.append(C1)\n",
        "\n",
        "                for j in range(i+1, len(X)):\n",
        "                    if X[j] in vocabulary and X[j] != cw1:\n",
        "                        for cw2 in close_words[X[j]]:\n",
        "                            if cw2 != X[j]:\n",
        "                                C2 = C1.copy()\n",
        "                                C2[j] = cw2\n",
        "                                CX.append(C2)\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "for W in CX:\n",
        "    print('\\t'.join(W))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT9-f0od7-Hr"
      },
      "source": [
        "Likelihood $P(X | W) = \\prod_{i=1}^n p(x_i | w_i)$ where $n$ is number of words in $X$ (same as in $W$), and $p(x | w)$  is Eq. B.8. $X$ is the written sentence, $W$ are the candidate sentences in $C(X)$. Each $W$ contains zero (ie, $W=X$) or at most one mispelled word, and in this case the mispelled word $w_i$ is at a Levenshtein distance $1...$ ``MAX_DIST`` of the written word $x_i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF4RPnLX7-Hs",
        "outputId": "8cc6065f-8629-4c71-d1cc-ca378d7466a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence with highest likelihood is the written one, X\n",
            "I\twish\tyou\twhere\there \t 0.7737809374999999\n",
            "Likelihoods\n",
            "I\twish\tyou\twhere\there \t 0.7737809374999999\n",
            "A\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "a\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "s\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "IV\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "If\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "In\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "It\twish\tyou\twhere\there \t 0.005817901785714291\n",
            "I\tfish\tyou\twhere\there \t 0.020362656250000017\n",
            "I\twith\tyou\twhere\there \t 0.020362656250000017\n",
            "I\twish\tYou\twhere\there \t 0.020362656250000017\n",
            "I\twish\tyour\twhere\there \t 0.020362656250000017\n",
            "I\twish\tyou\there\there \t 0.008145062500000006\n",
            "I\twish\tyou\twere\there \t 0.008145062500000006\n",
            "I\twish\tyou\tThere\there \t 0.008145062500000006\n",
            "I\twish\tyou\tWhere\there \t 0.008145062500000006\n",
            "I\twish\tyou\tthere\there \t 0.008145062500000006\n",
            "I\twish\tyou\twhere\ther \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\thers \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\twere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\tThere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\tWhere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\tthere \t 0.00581790178571429\n",
            "I\twish\tyou\twhere\twhere \t 0.00581790178571429\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.95\n",
        "likelihoods = []\n",
        "for W in CX:\n",
        "    PXW = 1.0\n",
        "    #print(X,W)\n",
        "    for x,w in zip(X,W):\n",
        "        if w==x:\n",
        "            pxw = alpha\n",
        "        else:\n",
        "            close_to_x = close_words[x] # includes x itself\n",
        "            pxw = (1-alpha) / (len(close_to_x) - 1) # so we substract 1\n",
        "        PXW *= pxw\n",
        "    likelihoods.append(PXW)\n",
        "\n",
        "likelihoods = np.array(likelihoods)\n",
        "idx_most_likely = likelihoods.argmax()\n",
        "print('Sentence with highest likelihood is the written one, X')\n",
        "print('\\t'.join(CX[idx_most_likely]), '\\t', likelihoods[idx_most_likely])\n",
        "print('Likelihoods')\n",
        "num_candidates = len(CX)\n",
        "for i in range(num_candidates):\n",
        "    print('\\t'.join(CX[i]), '\\t', likelihoods[i])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL_jue5E7-Hv"
      },
      "source": [
        "Priors $P(W)$ for all $W \\in C(X)$ computed by a LM model, for instance tri-grams (on this same corpus). We've chosen the stupid backoff version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19b20KJFbK37"
      },
      "source": [
        "Higher values of α assign more weight to the likelihood of correct words, while lower values of α assign more weight to the likelihood of misspelled words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kxkpVI1ZyL_",
        "outputId": "b4d3159e-2a9e-4e0d-e0ad-1701b58e0076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence with highest likelihood is the written one, X\n",
            "I\twish\tyou\twhere\there \t 0.32768000000000014\n",
            "Likelihoods\n",
            "I\twish\tyou\twhere\there \t 0.32768000000000014\n",
            "A\twish\tyou\twhere\there \t 0.011702857142857143\n",
            "a\twish\tyou\twhere\there \t 0.011702857142857143\n",
            "s\twish\tyou\twhere\there \t 0.011702857142857143\n",
            "IV\twish\tyou\twhere\there \t 0.011702857142857143\n",
            "If\twish\tyou\twhere\there \t 0.011702857142857143\n",
            "In\twish\tyou\twhere\there \t 0.011702857142857143\n",
            "It\twish\tyou\twhere\there \t 0.011702857142857143\n",
            "I\tfish\tyou\twhere\there \t 0.040959999999999996\n",
            "I\twith\tyou\twhere\there \t 0.040959999999999996\n",
            "I\twish\tYou\twhere\there \t 0.04096\n",
            "I\twish\tyour\twhere\there \t 0.04096\n",
            "I\twish\tyou\there\there \t 0.016384000000000003\n",
            "I\twish\tyou\twere\there \t 0.016384000000000003\n",
            "I\twish\tyou\tThere\there \t 0.016384000000000003\n",
            "I\twish\tyou\tWhere\there \t 0.016384000000000003\n",
            "I\twish\tyou\tthere\there \t 0.016384000000000003\n",
            "I\twish\tyou\twhere\ther \t 0.011702857142857143\n",
            "I\twish\tyou\twhere\thers \t 0.011702857142857143\n",
            "I\twish\tyou\twhere\twere \t 0.011702857142857143\n",
            "I\twish\tyou\twhere\tThere \t 0.011702857142857143\n",
            "I\twish\tyou\twhere\tWhere \t 0.011702857142857143\n",
            "I\twish\tyou\twhere\tthere \t 0.011702857142857143\n",
            "I\twish\tyou\twhere\twhere \t 0.011702857142857143\n"
          ]
        }
      ],
      "source": [
        "#ex3\n",
        "alpha = 0.8\n",
        "likelihoods = []\n",
        "for W in CX:\n",
        "    PXW = 1.0\n",
        "    #print(X,W)\n",
        "    for x,w in zip(X,W):\n",
        "        if w==x:\n",
        "            pxw = alpha\n",
        "        else:\n",
        "            close_to_x = close_words[x] # includes x itself\n",
        "            pxw = (1-alpha) / (len(close_to_x) - 1) # so we substract 1\n",
        "        PXW *= pxw\n",
        "    likelihoods.append(PXW)\n",
        "\n",
        "likelihoods = np.array(likelihoods)\n",
        "idx_most_likely = likelihoods.argmax()\n",
        "print('Sentence with highest likelihood is the written one, X')\n",
        "print('\\t'.join(CX[idx_most_likely]), '\\t', likelihoods[idx_most_likely])\n",
        "print('Likelihoods')\n",
        "num_candidates = len(CX)\n",
        "for i in range(num_candidates):\n",
        "    print('\\t'.join(CX[i]), '\\t', likelihoods[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64PapwMh7-Hw",
        "outputId": "d7f26e11-f0ac-439f-d2bc-50246fcd1d9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "100%|██████████| 7752/7752 [00:00<00:00, 14076.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<NgramCounter with 3 ngram orders and 623964 ngrams>\n"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "\n",
        "sents = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
        "text = []\n",
        "for s in tqdm(sents):\n",
        "    text.append(s[:-1]) # except ending point\n",
        "\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm.models import StupidBackoff\n",
        "\n",
        "n=3\n",
        "lm = StupidBackoff(alpha=0.4, order=n)\n",
        "train, vocab = padded_everygram_pipeline(n, text)\n",
        "lm.fit(train, vocab)\n",
        "print(lm.counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtUBjkAl7-Hy",
        "outputId": "28a86d31-3c44-4b3d-d6e2-4997df4df2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X = ['I', 'wish', 'you', 'where', 'here']\n",
            "['<s>', '<s>', 'I', 'wish', 'you', 'where', 'here', '</s>']\n",
            "P(I | <s>, <s>) = 0.08384932920536636\n",
            "P(wish | <s>, I) = 0.016923076923076923\n",
            "P(you | I, wish) = 0.34375\n",
            "P(where | wish, you) = 0.0002385211687537269\n",
            "P(here | you, where) = 9.863724853990915e-05\n",
            "P(X) = 1.1475972685804098e-11\n",
            "\n",
            "W = CX[1] = ['A', 'wish', 'you', 'where', 'here']\n",
            "['<s>', '<s>', 'A', 'wish', 'you', 'where', 'here', '</s>']\n",
            "P(A | <s>, <s>) = 0.011996904024767802\n",
            "P(wish | <s>, A) = 9.937888198757765e-05\n",
            "P(you | A, wish) = 0.04477611940298508\n",
            "P(where | wish, you) = 0.0002385211687537269\n",
            "P(here | you, where) = 9.863724853990915e-05\n",
            "P(W) = 1.2559652429973822e-15\n"
          ]
        }
      ],
      "source": [
        "def P(W, verbose=False):\n",
        "    S = ['<s>', '<s>',] + W + ['</s>']\n",
        "    if verbose: print(S)\n",
        "    num_words = len(S)\n",
        "    PW = 1.0\n",
        "    for i in range(2,num_words-1): # omit </s> because likelihoods don't have it\n",
        "        score = lm.score(S[i], [S[i-2], S[i-1]])\n",
        "        if verbose: print('P({} | {}, {}) = {}'.format(S[i], S[i-2], S[i-1], score))\n",
        "        PW *= score\n",
        "    return PW\n",
        "\n",
        "print('X =', X)\n",
        "score = P(X, verbose=True)\n",
        "print('P(X) = {}'.format(score))\n",
        "print('\\nW = CX[1] = {}'.format(CX[1]))\n",
        "score = P(CX[1], verbose=True)\n",
        "print('P(W) = {}'.format(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vnyNJKDYHcK7"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbPXfszT7-Hz",
        "outputId": "338ee38d-41b5-4335-9a9c-a265c64f2c0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I\twish\tyou\twhere\there \tlikelihood=0.7737809374999999\tprior=-25.190765597926926\tposterior=-25.44723206986468 \n",
            "A\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=-34.31087200002036\tposterior=-39.45768760017987 \n",
            "a\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=-36.24830622321093\tposterior=-41.39512182337044 \n",
            "s\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=-35.70189587142808\tposterior=-40.848711471587585 \n",
            "IV\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=-inf\tposterior=-inf \n",
            "If\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=-34.53940639996945\tposterior=-39.68622200012896 \n",
            "In\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=-34.85448744660934\tposterior=-40.00130304676885 \n",
            "It\twish\tyou\twhere\there \tlikelihood=0.005817901785714291\tprior=-33.156496136833795\tposterior=-38.3033117369933 \n",
            "I\tfish\tyou\twhere\there \tlikelihood=0.020362656250000017\tprior=-40.154760378806564\tposterior=-44.0488130104707 \n",
            "I\twith\tyou\twhere\there \tlikelihood=0.020362656250000017\tprior=-31.67335194972399\tposterior=-35.567404581388125 \n",
            "I\twish\tYou\twhere\there \tlikelihood=0.020362656250000017\tprior=-34.034281235032985\tposterior=-37.92833386669712 \n",
            "I\twish\tyour\twhere\there \tlikelihood=0.020362656250000017\tprior=-29.099339221664348\tposterior=-32.99339185332848 \n",
            "I\twish\tyou\there\there \tlikelihood=0.008145062500000006\tprior=-26.07377469768554\tposterior=-30.88411806122383 \n",
            "I\twish\tyou\twere\there \tlikelihood=0.008145062500000006\tprior=-15.273556823911006\tposterior=-20.0839001874493 <----\n",
            "I\twish\tyou\tThere\there \tlikelihood=0.008145062500000006\tprior=-26.10431142154562\tposterior=-30.91465478508391 \n",
            "I\twish\tyou\tWhere\there \tlikelihood=0.008145062500000006\tprior=-28.191535103667512\tposterior=-33.001878467205806 \n",
            "I\twish\tyou\tthere\there \tlikelihood=0.008145062500000006\tprior=-24.09215330925882\tposterior=-28.902496672797113 \n",
            "I\twish\tyou\twhere\ther \tlikelihood=0.00581790178571429\tprior=-22.30583887966182\tposterior=-27.452654479821327 \n",
            "I\twish\tyou\twhere\thers \tlikelihood=0.00581790178571429\tprior=-27.08538245259469\tposterior=-32.23219805275419 \n",
            "I\twish\tyou\twhere\twere \tlikelihood=0.00581790178571429\tprior=-23.69929870874258\tposterior=-28.846114308902088 \n",
            "I\twish\tyou\twhere\tThere \tlikelihood=0.00581790178571429\tprior=-25.22130232178701\tposterior=-30.368117921946517 \n",
            "I\twish\tyou\twhere\tWhere \tlikelihood=0.00581790178571429\tprior=-27.3085260039089\tposterior=-32.4553416040684 \n",
            "I\twish\tyou\twhere\tthere \tlikelihood=0.00581790178571429\tprior=-20.452527432826475\tposterior=-25.59934303298598 \n",
            "I\twish\tyou\twhere\twhere \tlikelihood=0.00581790178571429\tprior=-25.818434849107366\tposterior=-30.965250449266872 \n",
            "\n",
            "The original sentence was\n",
            "\tI wish you where here\n",
            "The right sentence is\n",
            "\tI wish you were here\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-62-328e94caab48>:3: RuntimeWarning: divide by zero encountered in log\n",
            "  log_priors.append(np.log(P(W)))\n"
          ]
        }
      ],
      "source": [
        "#EX1\n",
        "log_priors = []\n",
        "for W in CX:\n",
        "    log_priors.append(np.log(P(W)))\n",
        "\n",
        "log_posteriors = np.array(log_priors)+np.array(np.log(likelihoods))\n",
        "idx_best_post = np.argmax(log_posteriors)\n",
        "\n",
        "for i in range(num_candidates):\n",
        "    best = '<----' if i==idx_best_post else ''\n",
        "    print('\\t'.join(CX[i]), '\\tlikelihood={}\\tprior={}\\tposterior={} {}'\n",
        "          .format(likelihoods[i], log_priors[i],log_posteriors[i],best))\n",
        "\n",
        "print('\\nThe original sentence was')\n",
        "print('\\t' + ' '.join(X))\n",
        "print('The right sentence is')\n",
        "print('\\t' + ' '.join(CX[idx_best_post]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3sy8OkVpXOw",
        "outputId": "c2e4b81e-771b-4dbd-987b-b846ebd3556e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21\n"
          ]
        }
      ],
      "source": [
        "#ex4\n",
        "sentences = ['They danger was here','I fish you where here','her father and herself were let do dine together', 'I will go do sleep', 'He was an bid man', 'He was a had man', 'Only too of us', 'Only too of up','I ill help your', 'I ill help you', 'A will miss you',\n",
        "             'It was true that her friend ways going only half a mild', 'It was true that her friend was doing only half am mile','You will she me again', 'You will seen my again','I am what your made we', 'I am what your made me',\n",
        "             'He is got may father', 'He is not may father','then your are nor my friend','then you are nor by friend']\n",
        "print(len(sentences))\n",
        "Xs = []\n",
        "CXs = []\n",
        "for sentence in sentences:\n",
        "  X = sentence.split(' ')\n",
        "  Xs.append(X)\n",
        "  for x in X:\n",
        "      assert x in vocabulary, 'All the words in the sentence must belong to the vocabulary, {} doesn\\'t'.format(x)\n",
        "\n",
        "  CX = [X]  # Original sentence\n",
        "\n",
        "  for i in range(len(X)):\n",
        "      if X[i] in vocabulary:\n",
        "          for cw1 in close_words[X[i]]:\n",
        "              if cw1 != X[i]:\n",
        "                  C1 = X.copy()\n",
        "                  C1[i] = cw1\n",
        "                  CX.append(C1)\n",
        "\n",
        "                  for j in range(i+1, len(X)):\n",
        "                      if X[j] in vocabulary and X[j] != cw1:\n",
        "                          for cw2 in close_words[X[j]]:\n",
        "                              if cw2 != X[j]:\n",
        "                                  C2 = C1.copy()\n",
        "                                  C2[j] = cw2\n",
        "                                  CX.append(C2)\n",
        "      else:\n",
        "          pass\n",
        "  CXs.append(CX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "gVO_G-Ajuf5P"
      },
      "outputs": [],
      "source": [
        "alpha = 0.95\n",
        "Likelihoods = []\n",
        "for CX in CXs:\n",
        "  likelihoods = []\n",
        "  for W in CX:\n",
        "      PXW = 1.0\n",
        "      #print(X,W)\n",
        "      for x,w in zip(X,W):\n",
        "          if w==x:\n",
        "              pxw = alpha\n",
        "          else:\n",
        "              close_to_x = close_words[x] # includes x itself\n",
        "              pxw = (1-alpha) / (len(close_to_x) - 1) # so we substract 1\n",
        "          PXW *= pxw\n",
        "      likelihoods.append(PXW)\n",
        "\n",
        "  likelihoods = np.array(likelihoods)\n",
        "  Likelihoods.append(likelihoods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ubxz9QYvyc7",
        "outputId": "8f476270-d238-473f-9320-254916d00b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The original sentence was\n",
            "\tThey danger was here\n",
            "The right sentence is\n",
            "\tThe danger was her\n",
            "\n",
            "The original sentence was\n",
            "\tI fish you where here\n",
            "The right sentence is\n",
            "\tI wish you were here\n",
            "\n",
            "The original sentence was\n",
            "\ther father and herself were let do dine together\n",
            "The right sentence is\n",
            "\ther father and herself were left to dine together\n",
            "\n",
            "The original sentence was\n",
            "\tI will go do sleep\n",
            "The right sentence is\n",
            "\tI will go to sleep\n",
            "\n",
            "The original sentence was\n",
            "\tHe was an bid man\n",
            "The right sentence is\n",
            "\tHe was a bad man\n",
            "\n",
            "The original sentence was\n",
            "\tHe was a had man\n",
            "The right sentence is\n",
            "\tHe was a sad an\n",
            "\n",
            "The original sentence was\n",
            "\tOnly too of us\n",
            "The right sentence is\n",
            "\tOnly to of us\n",
            "\n",
            "The original sentence was\n",
            "\tOnly too of up\n",
            "The right sentence is\n",
            "\tOnly to of us\n",
            "\n",
            "The original sentence was\n",
            "\tI ill help your\n",
            "The right sentence is\n",
            "\tI will help you\n",
            "\n",
            "The original sentence was\n",
            "\tI ill help you\n",
            "The right sentence is\n",
            "\tI will help you\n",
            "\n",
            "The original sentence was\n",
            "\tA will miss you\n",
            "The right sentence is\n",
            "\tI will Miss you\n",
            "\n",
            "The original sentence was\n",
            "\tIt was true that her friend ways going only half a mild\n",
            "The right sentence is\n",
            "\tIt was true that her friend was going only half a mile\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-92-4610e3dea6fe>:4: RuntimeWarning: divide by zero encountered in log\n",
            "  log_priors.append(np.log(P(W)))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The original sentence was\n",
            "\tIt was true that her friend was doing only half am mile\n",
            "The right sentence is\n",
            "\tIt was true that her friend was going only half a mile\n",
            "\n",
            "The original sentence was\n",
            "\tYou will she me again\n",
            "The right sentence is\n",
            "\tYou will he be again\n",
            "\n",
            "The original sentence was\n",
            "\tYou will seen my again\n",
            "The right sentence is\n",
            "\tYou will see me again\n",
            "\n",
            "The original sentence was\n",
            "\tI am what your made we\n",
            "The right sentence is\n",
            "\tI am that you made we\n",
            "\n",
            "The original sentence was\n",
            "\tI am what your made me\n",
            "The right sentence is\n",
            "\tI am that you made me\n",
            "\n",
            "The original sentence was\n",
            "\tHe is got may father\n",
            "The right sentence is\n",
            "\tHe is not my father\n",
            "\n",
            "The original sentence was\n",
            "\tHe is not may father\n",
            "The right sentence is\n",
            "\tHe is not my father\n",
            "\n",
            "The original sentence was\n",
            "\tthen your are nor my friend\n",
            "The right sentence is\n",
            "\tWhen you are nor my friend\n",
            "\n",
            "The original sentence was\n",
            "\tthen you are nor by friend\n",
            "The right sentence is\n",
            "\tWhen you are not by friend\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(CXs)):\n",
        "  log_priors = []\n",
        "  for W in CXs[i]:\n",
        "      log_priors.append(np.log(P(W)))\n",
        "\n",
        "  log_posteriors = np.array(log_priors)+np.array(np.log(Likelihoods[i]))\n",
        "  idx_best_post = np.argmax(log_posteriors)\n",
        "\n",
        "  print('\\nThe original sentence was')\n",
        "  print('\\t' + ' '.join(Xs[i]))\n",
        "  print('The right sentence is')\n",
        "  print('\\t' + ' '.join(CXs[i][idx_best_post]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "cgfQsoce1ilc"
      },
      "outputs": [],
      "source": [
        "#ex5\n",
        "sentences = ['The danger was here','then you are not my friend','He is my father','I am what you made me','I will miss you','She was my sister','I will go to sleep','He is a bad man']\n",
        "Xs = []\n",
        "CXs = []\n",
        "for sentence in sentences:\n",
        "  X = sentence.split(' ')\n",
        "  Xs.append(X)\n",
        "  for x in X:\n",
        "      assert x in vocabulary, 'All the words in the sentence must belong to the vocabulary, {} doesn\\'t'.format(x)\n",
        "\n",
        "  CX = [X]  # Original sentence\n",
        "\n",
        "  for i in range(len(X)):\n",
        "      if X[i] in vocabulary:\n",
        "          for cw1 in close_words[X[i]]:\n",
        "              if cw1 != X[i]:\n",
        "                  C1 = X.copy()\n",
        "                  C1[i] = cw1\n",
        "                  CX.append(C1)\n",
        "\n",
        "                  for j in range(i+1, len(X)):\n",
        "                      if X[j] in vocabulary and X[j] != cw1:\n",
        "                          for cw2 in close_words[X[j]]:\n",
        "                              if cw2 != X[j]:\n",
        "                                  C2 = C1.copy()\n",
        "                                  C2[j] = cw2\n",
        "                                  CX.append(C2)\n",
        "      else:\n",
        "          pass\n",
        "  CXs.append(CX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-JthE3P3rip",
        "outputId": "9276761d-deee-44b8-f0ce-11383f2449f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The original sentence was\n",
            "\tThe danger was here\n",
            "The right sentence is\n",
            "\tThe danger as her\n",
            "\n",
            "The original sentence was\n",
            "\tthen you are not my friend\n",
            "The right sentence is\n",
            "\tWhen you are got my friend\n",
            "\n",
            "The original sentence was\n",
            "\tHe is my father\n",
            "The right sentence is\n",
            "\tHe is my father\n",
            "\n",
            "The original sentence was\n",
            "\tI am what you made me\n",
            "The right sentence is\n",
            "\tI am that you make me\n",
            "\n",
            "The original sentence was\n",
            "\tI will miss you\n",
            "The right sentence is\n",
            "\tI will Miss you\n",
            "\n",
            "The original sentence was\n",
            "\tShe was my sister\n",
            "The right sentence is\n",
            "\tShe was my sister\n",
            "\n",
            "The original sentence was\n",
            "\tI will go to sleep\n",
            "The right sentence is\n",
            "\tI will go to sleep\n",
            "\n",
            "The original sentence was\n",
            "\tHe is a bad man\n",
            "The right sentence is\n",
            "\tHe is a bad man\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-94-793cd4a22b85>:24: RuntimeWarning: divide by zero encountered in log\n",
            "  log_priors.append(np.log(P(W)))\n"
          ]
        }
      ],
      "source": [
        "alpha = 0.95\n",
        "Likelihoods = []\n",
        "for CX in CXs:\n",
        "  likelihoods = []\n",
        "  for W in CX:\n",
        "      PXW = 1.0\n",
        "      #print(X,W)\n",
        "      for x,w in zip(X,W):\n",
        "          if w==x:\n",
        "              pxw = alpha\n",
        "          else:\n",
        "              close_to_x = close_words[x] # includes x itself\n",
        "              pxw = (1-alpha) / (len(close_to_x) - 1) # so we substract 1\n",
        "          PXW *= pxw\n",
        "      likelihoods.append(PXW)\n",
        "\n",
        "  likelihoods = np.array(likelihoods)\n",
        "  Likelihoods.append(likelihoods)\n",
        "\n",
        "\n",
        "for i in range(len(CXs)):\n",
        "  log_priors = []\n",
        "  for W in CXs[i]:\n",
        "      log_priors.append(np.log(P(W)))\n",
        "\n",
        "  log_posteriors = np.array(log_priors)+np.array(np.log(Likelihoods[i]))\n",
        "  idx_best_post = np.argmax(log_posteriors)\n",
        "\n",
        "  print('\\nThe original sentence was')\n",
        "  print('\\t' + ' '.join(Xs[i]))\n",
        "  print('The right sentence is')\n",
        "  print('\\t' + ' '.join(CXs[i][idx_best_post]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkNoyjTG4ULH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFbhoCLL47zW"
      },
      "source": [
        "That's it! Now, let's try this code and do some modifications:\n",
        "1. change probabilities by log of probabilities to get scores more easy to interpret\n",
        "1. relax the assumption that in a sentence there's at most 1 mispelled word, what if we suppose there may be at most 2 ?\n",
        "1. and what is the effect of $\\alpha$ ? change it to 0.9, 0.8...\n",
        "1. make a long (>20) list of sentences each with 1 or 2 mispelled words to assess the speller : does it really work ?\n",
        "1. same for a second list of sentences withouth any spelling error (but different from any one in the selected part of the book), are there any errors ?\n",
        "1. extra points: what if you change the tri-gram language model by the neural network of exercise 2 ?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ngrams",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a5f976fab99073dff0a6bed391851c6449ba69de3700e06fc73b9df6f40e665b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
