{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0V5wnM_54gV",
        "outputId": "6ea38e5d-2c6a-492d-d57d-bc6de6a1eb57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import nltk\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZLuC4R7tSjj",
        "outputId": "2d019313-7ee7-42ff-c3aa-a5b456241e10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD8byP7V54gb",
        "outputId": "455a4f71-fa67-4ccf-e114-23345567f23a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 98552/98552 [00:05<00:00, 19469.10it/s]\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import gutenberg as corpus\n",
        "\n",
        "words = []\n",
        "#words_to_remove = ['*0*', '-Fpa-', '-Fpt-']\n",
        "#for s in tqdm(corpus.sents()[:1000]): # debug or quickly train the network\n",
        "for s in tqdm(corpus.sents()):\n",
        "    new_s = ['<s>'] + s[:-1] + ['</s>']\n",
        "#    new_s = [w for w in new_s if w not in words_to_remove]\n",
        "    words.extend(new_s)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyWhB_FG54ge",
        "outputId": "505d8cfe-0c8a-4a69-a088-324428482aa9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class FixedWindow(Dataset):\n",
        "    def __init__(self, words, length_window):\n",
        "        super().__init__()\n",
        "        self.length_window = length_window\n",
        "        \n",
        "        self.vocab = set()\n",
        "        for word in words:\n",
        "          self.vocab.add(word)\n",
        "        self.vocab = list(self.vocab)\n",
        "        self.vocabulary_size = len(self.vocab)\n",
        "        self.id2word = {id:word for id,word in enumerate(self.vocab)}\n",
        "        self.word2id = {word:id for id,word in enumerate(self.vocab)}\n",
        "        self.ids=[]\n",
        "        for word in words:\n",
        "          self.ids.append(self.word2id[word])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids) - self.length_window\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        first_ids = torch.tensor(self.ids[idx:idx+self.length_window-1])\n",
        "        last_id = torch.tensor(self.ids[idx+self.length_window-1])\n",
        "\n",
        "        return first_ids, last_id\n",
        "\n",
        "\n",
        "\n",
        "length_window = 5\n",
        "dataset = FixedWindow(words, length_window)\n",
        "\n",
        "x, y = dataset.__getitem__(10)\n",
        "print('x = {}, y = {}'.format(x, y))\n",
        "\n",
        "batch_size = 1000 # 5 to debug\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) # shuffle=False to debug\n",
        "\n",
        "if True:\n",
        "    for nbatch, (X, y) in enumerate(dataloader):\n",
        "        print('batch {}'.format(nbatch))\n",
        "        print('X = {}'.format(X))\n",
        "        print('y = {}'.format(y))\n",
        "        for x,z in zip(X.numpy(), y.numpy()):\n",
        "            print([dataset.id2word[w] for w in x], end=' ')\n",
        "            print(dataset.id2word[z])\n",
        "        if nbatch==3:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XTnVK2qA54gi"
      },
      "outputs": [],
      "source": [
        "class NNLM(nn.Module):\n",
        "    def __init__(self, num_classes, dim_input, dim_hidden, dim_embedding):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.dim_input = dim_input\n",
        "        self.dim_hidden = dim_hidden\n",
        "        self.dim_embedding = dim_embedding\n",
        "        self.embeddings = nn.Embedding(self.num_classes, self.dim_embedding) # embedding layer or look up table\n",
        "        self.hidden1 = nn.Linear(self.dim_input * self.dim_embedding, self.dim_hidden, bias=False)\n",
        "        self.ones = nn.Parameter(torch.ones(self.dim_hidden))\n",
        "        self.hidden2 = nn.Linear(self.dim_hidden, self.num_classes, bias=False)\n",
        "        self.hidden3 = nn.Linear(self.dim_input * self.dim_embedding, self.num_classes, bias=False) # final layer\n",
        "        self.bias = nn.Parameter(torch.ones(self.num_classes))\n",
        "\n",
        "    def forward(self, X):\n",
        "        word_embeds = self.embeddings(X)\n",
        "        X = word_embeds.view(-1, self.dim_input * self.dim_embedding) # first layer\n",
        "        tanh = torch.tanh(self.ones + self.hidden1(X)) # tanh layer\n",
        "        output = self.bias + self.hidden3(X) + self.hidden2(tanh) # summing up all the layers with bias\n",
        "        return output\n",
        "\n",
        "\n",
        "num_classes = dataset.vocabulary_size\n",
        "dim_input = length_window - 1\n",
        "dim_hidden = 50\n",
        "dim_embedding = 32\n",
        "learning_rate= 1e-3\n",
        "num_epochs = 30\n",
        "\n",
        "model = NNLM(num_classes, dim_input, dim_hidden, dim_embedding)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "path = 'NNLM.pt'\n",
        "do_train = True\n",
        "do_test = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISE7W8MG9niv",
        "outputId": "fe30602d-73bd-4d95-da68-7ce37ddc8d2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# In the top menu go to Runtime -> Change runtime type and set Hardware\n",
        "# accelerator to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pENOQtyI54gl",
        "outputId": "0de7882e-2ea5-4e55-867a-b0bc97472626"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.random import device_count\n",
        "if do_train:\n",
        "    size = len(dataloader.dataset)\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            loss = loss_fn(pred, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                loss, current = loss.item(), batch * batch_size\n",
        "                print('Epoch {} loss: {:>7f}  [{:>5d}/{:>5d}]'\n",
        "                    .format(epoch+1, loss, current, size))\n",
        "\n",
        "    torch.save({'model_state_dict': model.state_dict()}, path)\n",
        "else:\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrstu81q54gm",
        "outputId": "a830de99-bc18-4dc8-e5c3-addf950cd572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The day that the Greenland whale was gone mad gaping activity, and in spite of him watch, ' appeared - separate here yet hitherto most comical things prior. There is another to whom ; and therefore pour out drink out the dreadful gulf, And spotted, hidden herds. So out of the others, he said, let. 3 : 2 Therefore, O Aholibah, to morrow after all, in the prophecy of the kings of Israel, and to build an house away that was in the\n"
          ]
        }
      ],
      "source": [
        "if do_test:\n",
        "    num_sentences = 5\n",
        "    max_num_words = 100\n",
        "\n",
        "    nsent = 0\n",
        "    generated_words = ['<s>', 'The', 'day', 'that']\n",
        "    assert len(generated_words)==dim_input # length_window-1\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        while (nsent < num_sentences) and (len(generated_words) < max_num_words):\n",
        "            input_ids = None\n",
        "            \n",
        "            input_ids = [dataset.word2id[word] for word in generated_words[-dim_input:]]\n",
        "\n",
        "            pred = model(torch.tensor(input_ids).unsqueeze(0).to(device))\n",
        "            probs = torch.nn.functional.softmax(pred, dim=1)\n",
        "\n",
        "            \n",
        "            output_id=torch.multinomial(probs, 1)\n",
        "            output_word = None\n",
        "            \n",
        "            output_word = dataset.id2word[output_id.item()]\n",
        "            generated_words += [output_word]\n",
        "            if output_word=='</s>':\n",
        "                nsent += 1\n",
        "\n",
        "    generated_text = ' '.join(generated_words)\n",
        "    generated_text = generated_text.replace(' </s> <s>', '.').replace('<s> ','').replace(' </s>','.')\n",
        "    for s in [' l\\' ',' s\\' ',' d\\' ',]:\n",
        "        generated_text = generated_text.replace(s, s[:-1])\n",
        "    generated_text = generated_text.replace(' , ', ', ').replace('_',' ')\n",
        "    print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
